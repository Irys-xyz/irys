#!/usr/bin/env python3
"""
Main partition sync test - comprehensive test that replicates slow_heavy_sync_partition_data_between_peers_test

This is the primary test script for validating partition data synchronization across Irys nodes.
It performs thorough testing of data transaction submission, chunk uploads, and partition sync.
"""

import asyncio
import aiohttp
import json
import time
import logging
import hashlib
import secrets
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import struct

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NodeConfig:
    name: str
    url: str
    mining_key: str
    reward_address: str

@dataclass 
class ChunkCounts:
    data: int
    packed: int

class IrysComprehensiveTestClient:
    def __init__(self):
        self.nodes = {
            'test-irys-1': NodeConfig('test-irys-1', 'http://localhost:9080', 
                               '53142d3c5a4a3f49715490456c26df7926d45586179d19afce68d799c08ea8c6',
                               '0x10605A299777D44BE5373D120d5479f07860325d'),
            'test-irys-2': NodeConfig('test-irys-2', 'http://localhost:9081',
                               '4882545ed67d1b638207334975ee839e2369d31d8b3b8ef656b94952ca4ae4f3', 
                               '0x951AA3768fE8d51DbC348520b0825C0c1f197277'),
            'test-irys-3': NodeConfig('test-irys-3', 'http://localhost:9082',
                               '7821345ed67d1b638207334975ee839e2369d31d8b3b8ef656b94952ca4ae7f9',
                               '0x852BB3678fE8d51DbC348520b0825C0c1f197288')
        }
        self.session = None
        self.genesis_node = 'test-irys-1'
        self.peer_nodes = ['test-irys-2', 'test-irys-3']
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()

    async def get_node_info(self, node_name: str) -> Dict:
        """Get node information"""
        node = self.nodes[node_name]
        async with self.session.get(f"{node.url}/v1/info") as resp:
            resp.raise_for_status()
            return await resp.json()

    async def get_chain_height(self, node_name: str) -> int:
        """Get current chain height"""
        node = self.nodes[node_name]
        async with self.session.get(f"{node.url}/v1/observability/chain/height") as resp:
            resp.raise_for_status()
            data = await resp.json()
            return data['height']

    async def wait_for_height(self, node_name: str, target_height: int, timeout: int = 60):
        """Wait for node to reach target height"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            current_height = await self.get_chain_height(node_name)
            if current_height >= target_height:
                logger.info(f"{node_name} reached height {current_height}")
                return current_height
            await asyncio.sleep(1)
        raise TimeoutError(f"{node_name} didn't reach height {target_height} in {timeout}s")

    def create_data_root(self, data: bytes) -> str:
        """Create merkle root from data chunks"""
        return "0x" + hashlib.sha256(data).hexdigest()

    def create_transaction_id(self, data_root: str, signer: str, data_size: int) -> str:
        """Create transaction ID from header components"""
        components = f"{data_root}{signer}{data_size}"
        return "0x" + hashlib.sha256(components.encode()).hexdigest()

    async def create_proper_data_transaction(self, node_name: str, data: bytes) -> Dict:
        """Create a properly formatted data transaction header"""
        node = self.nodes[node_name]
        data_root = self.create_data_root(data)
        
        # Get recent block hash for anchor
        try:
            genesis_height = await self.get_chain_height(node_name)
            anchor = "0x" + "1" * 64  # Use dummy anchor for now
        except:
            anchor = "0x" + "1" * 64

        # Create a fake signature structure (try hex format instead of base58)
        fake_signature_hex = "0x" + secrets.token_hex(64)  # 64 bytes as hex
        
        # Use the hex address directly
        signer_hex = node.reward_address

        tx_header = {
            "version": 1,
            "anchor": anchor,
            "signer": signer_hex,  # Hex-encoded address
            "dataRoot": data_root,
            "dataSize": str(len(data)),
            "headerSize": "0",
            "termFee": "1000000000000000",  # 0.001 ETH in wei as string
            "ledgerId": 0,  # Publish ledger 
            "chainId": "1270",
            "signature": fake_signature_hex,  # Hex encoded signature
            "bundleFormat": None,
            "permFee": None
        }
        
        # Calculate transaction ID
        tx_id = self.create_transaction_id(data_root, node.reward_address, len(data))
        return {"header": tx_header, "id": tx_id, "data_root": data_root}

    async def post_data_transaction(self, node_name: str, data: bytes) -> Dict:
        """Post data transaction and return transaction info"""
        node = self.nodes[node_name]
        tx_info = await self.create_proper_data_transaction(node_name, data)
        
        try:
            # Debug: print what we're sending
            import json as json_module
            logger.info(f"DEBUG: Sending transaction header: {json_module.dumps(tx_info['header'], indent=2)}")
            
            async with self.session.post(f"{node.url}/v1/tx", json=tx_info["header"]) as resp:
                response_text = await resp.text()
                if resp.status == 200:
                    logger.info(f"‚úÖ Posted data transaction to {node_name}")
                    return tx_info
                else:
                    logger.error(f"‚ùå Failed to post data tx to {node_name}: {resp.status}")
                    logger.error(f"Response: {response_text}")
                    raise Exception(f"Transaction failed: {resp.status}")
        except Exception as e:
            logger.error(f"Exception posting data tx to {node_name}: {e}")
            raise

    async def post_chunk(self, node_name: str, tx_info: Dict, chunk_index: int, chunks: List[bytes]):
        """Upload a data chunk"""
        node = self.nodes[node_name]
        chunk_data = chunks[chunk_index]
        
        chunk_payload = {
            "dataRoot": tx_info["data_root"],
            "dataSize": len(b''.join(chunks)),
            "dataPath": "0x" + "0" * 64,  # Simplified merkle path
            "bytes": chunk_data.hex(),
            "txOffset": chunk_index * 32  # 32 bytes per chunk
        }
        
        try:
            async with self.session.post(f"{node.url}/v1/chunk", json=chunk_payload) as resp:
                if resp.status == 200:
                    logger.debug(f"‚úÖ Posted chunk {chunk_index} to {node_name}")
                else:
                    response_text = await resp.text()
                    logger.warning(f"‚ö†Ô∏è  Failed to post chunk {chunk_index} to {node_name}: {resp.status}")
                    logger.debug(f"Response: {response_text}")
        except Exception as e:
            logger.warning(f"Exception posting chunk {chunk_index} to {node_name}: {e}")

    async def create_commitment_transaction(self, node_name: str, commitment_type: str) -> Dict:
        """Create stake or pledge commitment transaction"""
        node = self.nodes[node_name]
        
        # Create fake signature in hex format
        fake_signature_hex = "0x" + secrets.token_hex(64)  # 64 bytes as hex
        
        # Use the hex address directly
        signer_hex = node.reward_address
        
        # Determine commitment type and value
        if commitment_type == "Stake":
            commitment_type_field = "Stake"
            value = "1000000000000000000"  # 1 ETH for stake
        else:  # Pledge
            commitment_type_field = {"Pledge": {"pledgeCountBeforeExecuting": 0}}
            value = "100000000000000000"  # 0.1 ETH for pledge
        
        commitment_tx = {
            "version": 1,
            "anchor": "0x" + "1" * 64,
            "signer": signer_hex,  # Hex-encoded address
            "commitmentType": commitment_type_field,
            "chainId": "1270",
            "fee": "1000000000000000",  # Fee in wei
            "value": value,  # Value in wei
            "signature": fake_signature_hex
        }
        
        return commitment_tx

    async def post_stake_commitment(self, node_name: str) -> bool:
        """Post stake commitment transaction"""
        node = self.nodes[node_name]
        stake_tx = await self.create_commitment_transaction(node_name, "Stake")
        
        try:
            async with self.session.post(f"{node.url}/v1/commitment_tx", json=stake_tx) as resp:
                if resp.status == 200:
                    logger.info(f"‚úÖ Posted stake commitment for {node_name}")
                    return True
                else:
                    response_text = await resp.text()
                    logger.warning(f"‚ö†Ô∏è  Failed to post stake for {node_name}: {resp.status}")
                    logger.debug(f"Response: {response_text}")
                    return False
        except Exception as e:
            logger.warning(f"Exception posting stake for {node_name}: {e}")
            return False

    async def post_pledge_commitment(self, node_name: str) -> bool:
        """Post pledge commitment transaction"""
        node = self.nodes[node_name]
        pledge_tx = await self.create_commitment_transaction(node_name, "Pledge")
        
        try:
            async with self.session.post(f"{node.url}/v1/commitment_tx", json=pledge_tx) as resp:
                if resp.status == 200:
                    logger.debug(f"‚úÖ Posted pledge commitment for {node_name}")
                    return True
                else:
                    response_text = await resp.text()
                    logger.debug(f"‚ö†Ô∏è  Failed to post pledge for {node_name}: {resp.status}")
                    return False
        except Exception as e:
            logger.debug(f"Exception posting pledge for {node_name}: {e}")
            return False

    async def get_storage_intervals(self, node_name: str, ledger: str, slot_index: int, chunk_type: str) -> Dict:
        """Get storage intervals for monitoring sync"""
        node = self.nodes[node_name]
        url = f"{node.url}/v1/observability/storage/intervals/{ledger}/{slot_index}/{chunk_type}"
        
        try:
            async with self.session.get(url) as resp:
                if resp.status == 200:
                    return await resp.json()
                else:
                    return {"intervals": []}
        except Exception:
            return {"intervals": []}

    async def check_chunk_counts(self, node_name: str, ledger: str, slot_index: int) -> ChunkCounts:
        """Check data and packed chunk counts for a node/ledger/slot"""
        data_intervals = await self.get_storage_intervals(node_name, ledger, slot_index, "Data")
        packed_intervals = await self.get_storage_intervals(node_name, ledger, slot_index, "Entropy")
        
        # Count chunks from intervals
        data_count = sum(interval.get("end", 0) - interval.get("start", 0) + 1 
                        for interval in data_intervals.get("intervals", []) if interval.get("start") is not None)
        packed_count = sum(interval.get("end", 0) - interval.get("start", 0) + 1 
                          for interval in packed_intervals.get("intervals", []) if interval.get("start") is not None)
        
        return ChunkCounts(data=data_count, packed=packed_count)

    async def wait_for_ingress_proofs(self, node_name: str, tx_id: str, timeout: int = 60):
        """Wait for transaction to be promoted via ingress proofs"""
        node = self.nodes[node_name]
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                async with self.session.get(f"{node.url}/v1/tx/{tx_id}/promotion") as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if data.get("isPromoted", False):
                            logger.info(f"‚úÖ Transaction {tx_id} promoted on {node_name}")
                            return True
            except Exception:
                pass
            
            await asyncio.sleep(2)
        
        logger.warning(f"‚ö†Ô∏è  Transaction {tx_id} not promoted within {timeout}s")
        return False

    async def mine_epoch_if_possible(self, node_name: str) -> bool:
        """Attempt to trigger epoch mining (may not be available via HTTP)"""
        node = self.nodes[node_name]
        
        # Try mining endpoint if available
        try:
            async with self.session.post(f"{node.url}/v1/mine/epoch") as resp:
                if resp.status == 200:
                    logger.info(f"‚úÖ Triggered epoch mining on {node_name}")
                    return True
        except Exception:
            pass
        
        # Try block mining endpoint
        try:
            async with self.session.post(f"{node.url}/v1/mine/block") as resp:
                if resp.status == 200:
                    logger.info(f"‚úÖ Triggered block mining on {node_name}")
                    return True
        except Exception:
            pass
        
        logger.debug(f"‚ö†Ô∏è  Mining endpoints not available on {node_name}")
        return False

async def run_partition_sync_test():
    """
    Comprehensive test replicating slow_heavy_sync_partition_data_between_peers_test:
    
    1. Configure network (3 partitions per slot, 1 ingress proof)
    2. Start genesis node, verify partition assignments  
    3. Post data tx that fills 90% of first submit slot
    4. Stake and pledge two peer nodes for next epoch
    5. Mine epoch block to grow submit ledger
    6. Verify genesis node partition assignments
    7. Start two peer nodes
    8. Verify all slots have 3 replicas
    9. Upload chunks and wait for promotion
    10. Start peers and sync
    11. Validate partition data synchronization
    """
    async with IrysComprehensiveTestClient() as client:
        logger.info("üöÄ Starting comprehensive partition data sync test")
        
        # Phase 1: Network Initialization and Validation
        logger.info("\nüìã Phase 1: Network Initialization")
        
        # Verify all nodes are accessible
        nodes_info = {}
        for node_name in client.nodes:
            try:
                info = await client.get_node_info(node_name)
                nodes_info[node_name] = info
                logger.info(f"‚úÖ {node_name}: connected, version={info.get('version', 'unknown')}")
            except Exception as e:
                logger.error(f"‚ùå Failed to connect to {node_name}: {e}")
                return False
        
        # Get initial chain heights
        initial_heights = {}
        for node_name in client.nodes:
            try:
                height = await client.get_chain_height(node_name)
                initial_heights[node_name] = height
                logger.info(f"üìä {node_name} initial height: {height}")
            except Exception as e:
                logger.error(f"‚ùå Failed to get height for {node_name}: {e}")
                return False
        
        # Phase 2: Genesis Node Setup and Data Transaction
        logger.info("\nüìã Phase 2: Genesis Node Setup and Data Transaction")
        
        # Create test data (50 chunks of 32 bytes each = 1600 bytes)
        num_chunks = 50
        chunk_size = 32
        chunks = []
        for i in range(num_chunks):
            chunks.append(bytes([i] * chunk_size))
        data = b''.join(chunks)
        
        logger.info(f"üì¶ Created test data: {len(data)} bytes ({num_chunks} chunks of {chunk_size} bytes)")
        
        # Post data transaction to genesis node
        try:
            tx_info = await client.post_data_transaction(client.genesis_node, data)
            logger.info(f"‚úÖ Posted data transaction {tx_info['id']} to genesis node")
        except Exception as e:
            logger.error(f"‚ùå Failed to post data transaction: {e}")
            return False
        
        # Phase 3: Peer Node Commitments  
        logger.info("\nüìã Phase 3: Peer Node Commitments")
        
        # Post stake and pledge commitments for peer nodes
        commitment_success = True
        for peer_node in client.peer_nodes:
            # Post stake commitment
            if await client.post_stake_commitment(peer_node):
                logger.info(f"‚úÖ Posted stake commitment for {peer_node}")
            else:
                logger.warning(f"‚ö†Ô∏è  Failed to post stake commitment for {peer_node}")
                commitment_success = False
            
            # Post 3 pledge commitments per peer (as per original test)
            pledge_count = 0
            for i in range(3):
                if await client.post_pledge_commitment(peer_node):
                    pledge_count += 1
                await asyncio.sleep(0.5)  # Small delay between pledges
            
            logger.info(f"‚úÖ Posted {pledge_count}/3 pledge commitments for {peer_node}")
        
        if not commitment_success:
            logger.warning("‚ö†Ô∏è  Some commitment transactions failed, continuing test...")
        
        # Phase 4: Epoch Mining and Progression
        logger.info("\nüìã Phase 4: Epoch Mining and Chain Progression")
        
        # Attempt to trigger epoch mining
        genesis_height_before = await client.get_chain_height(client.genesis_node)
        
        # Try to mine epochs if mining endpoints are available
        for attempt in range(2):
            if await client.mine_epoch_if_possible(client.genesis_node):
                await asyncio.sleep(5)  # Wait for epoch processing
        
        # Wait for natural chain progression
        target_height = genesis_height_before + 8  # Wait for ~2 epochs worth of blocks
        logger.info(f"‚è≥ Waiting for chain progression from {genesis_height_before} to {target_height}")
        
        try:
            await client.wait_for_height(client.genesis_node, target_height, timeout=120)
            logger.info("‚úÖ Chain has progressed sufficiently")
        except TimeoutError:
            logger.warning("‚ö†Ô∏è  Chain progression slower than expected, continuing...")
        
        # Phase 5: Data Upload and Promotion
        logger.info("\nüìã Phase 5: Data Upload and Promotion")
        
        # Upload all chunks to genesis node
        logger.info(f"üì§ Uploading {num_chunks} chunks to genesis node...")
        upload_success_count = 0
        
        for chunk_index in range(num_chunks):
            try:
                await client.post_chunk(client.genesis_node, tx_info, chunk_index, chunks)
                upload_success_count += 1
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Failed to upload chunk {chunk_index}: {e}")
            
            if chunk_index % 10 == 9:  # Progress indicator
                logger.info(f"üìä Uploaded {chunk_index + 1}/{num_chunks} chunks")
        
        logger.info(f"‚úÖ Successfully uploaded {upload_success_count}/{num_chunks} chunks")
        
        # Wait for ingress proofs (transaction promotion)
        logger.info("‚è≥ Waiting for transaction promotion via ingress proofs...")
        promotion_success = await client.wait_for_ingress_proofs(client.genesis_node, tx_info["id"], timeout=60)
        
        if promotion_success:
            logger.info("‚úÖ Transaction promoted successfully")
        else:
            logger.warning("‚ö†Ô∏è  Transaction promotion not detected, continuing test...")
        
        # Phase 6: Comprehensive Data Synchronization Monitoring
        logger.info("\nüìã Phase 6: Data Synchronization Monitoring")
        
        # Expected final state (from original Rust test):
        # - Genesis node: Publish(0): data=50, packed=10 | Submit(0): data=50, packed=10 | Submit(1): data=0, packed=60
        # - Peer nodes: Same distribution after sync
        
        expected_counts = {
            ("Publish", 0): ChunkCounts(data=50, packed=10),
            ("Submit", 0): ChunkCounts(data=50, packed=10), 
            ("Submit", 1): ChunkCounts(data=0, packed=60)
        }
        
        sync_timeout = 60
        sync_success = False
        
        logger.info(f"üîç Monitoring data synchronization across all nodes (timeout: {sync_timeout}s)")
        
        for attempt in range(sync_timeout):
            await asyncio.sleep(1)
            
            all_nodes_synced = True
            sync_status = {}
            
            for node_name in client.nodes:
                node_synced = True
                node_status = {}
                
                for (ledger, slot_index), expected in expected_counts.items():
                    try:
                        actual = await client.check_chunk_counts(node_name, ledger, slot_index)
                        node_status[f"{ledger}({slot_index})"] = actual
                        
                        # Check if this ledger/slot matches expected
                        if actual.data != expected.data or actual.packed != expected.packed:
                            node_synced = False
                    
                    except Exception as e:
                        logger.debug(f"Error checking {node_name} {ledger}({slot_index}): {e}")
                        node_synced = False
                        node_status[f"{ledger}({slot_index})"] = ChunkCounts(data=0, packed=0)
                
                sync_status[node_name] = {"synced": node_synced, "counts": node_status}
                if not node_synced:
                    all_nodes_synced = False
            
            # Log progress every 10 seconds
            if attempt % 10 == 0 or all_nodes_synced:
                logger.info(f"\nüìä Sync Status (attempt {attempt + 1}):")
                for node_name, status in sync_status.items():
                    synced_indicator = "‚úÖ" if status["synced"] else "‚è≥"
                    logger.info(f"{synced_indicator} {node_name}:")
                    for ledger_slot, counts in status["counts"].items():
                        logger.info(f"   {ledger_slot}: data={counts.data}, packed={counts.packed}")
            
            if all_nodes_synced:
                sync_success = True
                logger.info(f"üéâ All nodes synchronized successfully at attempt {attempt + 1}")
                break
        
        # Phase 7: Final Validation and Summary
        logger.info("\nüìã Phase 7: Final Validation")
        
        # Check final chain heights
        final_heights = {}
        for node_name in client.nodes:
            try:
                height = await client.get_chain_height(node_name)
                final_heights[node_name] = height
                height_change = height - initial_heights.get(node_name, 0)
                logger.info(f"üìä {node_name} final height: {height} (Œî{height_change})")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Failed to get final height for {node_name}: {e}")
                final_heights[node_name] = 0
        
        # Validate height consistency
        height_values = list(final_heights.values())
        height_consistent = len(set(height_values)) <= 1  # Allow for minor differences
        
        if height_consistent:
            logger.info("‚úÖ All nodes are at consistent heights")
        else:
            logger.warning(f"‚ö†Ô∏è  Nodes at different heights: {final_heights}")
        
        # Final test result
        chain_progressed = max(height_values) > max(initial_heights.values())
        
        logger.info("\nüèÅ COMPREHENSIVE TEST RESULTS:")
        logger.info(f"‚úÖ Network connectivity: PASSED")
        logger.info(f"‚úÖ Data transaction posting: PASSED") 
        logger.info(f"{'‚úÖ' if commitment_success else '‚ö†Ô∏è '} Commitment transactions: {'PASSED' if commitment_success else 'PARTIAL'}")
        logger.info(f"{'‚úÖ' if chain_progressed else '‚ùå'} Chain progression: {'PASSED' if chain_progressed else 'FAILED'}")
        logger.info(f"{'‚úÖ' if upload_success_count == num_chunks else '‚ö†Ô∏è '} Chunk uploads: {upload_success_count}/{num_chunks}")
        logger.info(f"{'‚úÖ' if promotion_success else '‚ö†Ô∏è '} Transaction promotion: {'PASSED' if promotion_success else 'NOT_DETECTED'}")
        logger.info(f"{'‚úÖ' if sync_success else '‚ùå'} Data synchronization: {'PASSED' if sync_success else 'FAILED'}")
        logger.info(f"{'‚úÖ' if height_consistent else '‚ö†Ô∏è '} Height consistency: {'PASSED' if height_consistent else 'INCONSISTENT'}")
        
        overall_success = (
            chain_progressed and 
            (upload_success_count >= num_chunks * 0.8) and  # At least 80% uploads
            sync_success and
            height_consistent
        )
        
        if overall_success:
            logger.info("üéâ OVERALL RESULT: ‚úÖ SUCCESS - Test successfully replicated Rust test functionality")
            return True
        else:
            logger.info("‚ùå OVERALL RESULT: PARTIAL/FAILED - Some test phases did not complete successfully")
            return False

def main():
    """Main entry point for the comprehensive partition sync test"""
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description='Comprehensive Irys Partition Data Sync Test')
    parser.add_argument('--verbose', '-v', action='store_true', help='Enable verbose logging')
    parser.add_argument('--timeout', type=int, default=60, help='Sync timeout in seconds (default: 60)')
    parser.add_argument('--chunks', type=int, default=50, help='Number of test chunks to create (default: 50)')
    args = parser.parse_args()
    
    # Configure logging based on verbosity
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG, 
                          format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    else:
        logging.basicConfig(level=logging.INFO,
                          format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Store settings for test to use (would need to pass these to the test function)
    logger.info("Starting Comprehensive Irys Partition Sync Test")
    logger.info(f"Settings: timeout={args.timeout}s, chunks={args.chunks}, verbose={args.verbose}")
    
    try:
        success = asyncio.run(run_partition_sync_test())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("\nTest interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"Test failed with error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()